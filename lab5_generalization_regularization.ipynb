{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/porterjenkins/byu-cs474/blob/master/lab5_generalization_regularization.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ttC8f3uOlC2C"
   },
   "source": [
    "# Lab 5: Overfitting, Generalization, and Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dPQqEicJlNGa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "\n",
    "*   Understand how to accurately measure generalization performance of deep networks\n",
    "*   Gain intuition into the bias-variance trade-off and the double descent phenomenon\n",
    "*   Investigate properties of high dimensional spaces and better understand the ''curse of dimensionality''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDJ1RdGomDHj"
   },
   "source": [
    "## Deliverable\n",
    "\n",
    "You will turn in a completed version of notebook to Canvas/Learning Suite.  In various places you will see the words \"TO DO\". Follow the instructions at these places and write code to complete the instructions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RrGvYA4pkxH-"
   },
   "source": [
    "## Notes\n",
    "You will need a GPU instance for this lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "joIT4SFmmeFp"
   },
   "source": [
    "## Q1) Understanding Generalization\n",
    "\n",
    "In this question, we will use the MNIST1D dataset (https://github.com/greydanus/mnist1d) to reproduce Figure 8.2 from Prince."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cSu7e4Jmtfm"
   },
   "outputs": [],
   "source": [
    "# Run this if you're in a Colab to install MNIST 1D repository\n",
    "%pip install git+https://github.com/greydanus/mnist1d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 9387,
     "status": "ok",
     "timestamp": 1721938999131,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "3z76SO4ymxbW"
   },
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist1d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vDJ27aiZnRa-"
   },
   "source": [
    "Let's generate a training and test dataset using the MNIST1D code. The dataset gets saved as a .pkl file so it doesn't have to be regenerated each time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7271,
     "status": "ok",
     "timestamp": 1721939006399,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "Nh2VSEZHnSlo",
    "outputId": "dafa4a04-b658-456f-ca17-034d92d2fe1a"
   },
   "outputs": [],
   "source": [
    "!mkdir ./sample_data\n",
    "\n",
    "args = mnist1d.data.get_dataset_args()\n",
    "data = mnist1d.data.get_dataset(args, path='./sample_data/mnist1d_data.pkl', download=False, regenerate=False)\n",
    "\n",
    "# The training and test input and outputs are in\n",
    "# data['x'], data['y'], data['x_test'], and data['y_test']\n",
    "print(\"Examples in training set: {}\".format(len(data['y'])))\n",
    "print(\"Examples in test set: {}\".format(len(data['y_test'])))\n",
    "print(\"Length of each example: {}\".format(data['x'].shape[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1721939006399,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "qFiZ1dY9nUcO"
   },
   "outputs": [],
   "source": [
    "D_i = 40    # Input dimensions\n",
    "D_k = 100   # Hidden dimensions\n",
    "D_o = 10    # Output dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpU3rh4KuPHv"
   },
   "source": [
    "### Part 1.a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBmIfg5veT9S"
   },
   "source": [
    "**TODO:** Define a PyTorch model with two hidden layers of size 100 And ReLU activations between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1721939006641,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "iPSyIAj0rDqA"
   },
   "outputs": [],
   "source": [
    "# Your code here (see Figure 7.8 of book for help)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj_PwXFXekFA"
   },
   "source": [
    "**TODO:** Initialize the parameters with He initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1721939006641,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "S94b51qnoQU2"
   },
   "outputs": [],
   "source": [
    "def weights_init(layer_in):\n",
    "    # TODO: Replace this line (see figure 7.8 of Prince for help)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1721939006641,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "5Rf4hB4RoZpg",
    "outputId": "b8fcf526-1512-4dc6-d31e-e18f33f520f6"
   },
   "outputs": [],
   "source": [
    "# Call the function you just defined\n",
    "model.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12798,
     "status": "ok",
     "timestamp": 1721939019438,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "NbNIYtj7nZQj",
    "outputId": "dd22db91-07e5-41fb-cca8-9e94b9183cb8"
   },
   "outputs": [],
   "source": [
    "x_train = torch.tensor(data['x'].astype('float32'))\n",
    "y_train = torch.tensor(data['y'].transpose().astype('long'))\n",
    "x_test= torch.tensor(data['x_test'].astype('float32'))\n",
    "y_test = torch.tensor(data['y_test'].astype('long'))\n",
    "\n",
    "# load the data into a class that creates the batches\n",
    "data_loader = DataLoader(TensorDataset(x_train,y_train), batch_size=100, shuffle=True, worker_init_fn=np.random.seed(1))\n",
    "\n",
    "# Initialize model weights\n",
    "model.apply(weights_init)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, n_epoch=50):\n",
    "    \n",
    "    # Initialize model weights\n",
    "    model.apply(weights_init)\n",
    "\n",
    "    # store the loss and the % correct at each epoch\n",
    "    losses_train = np.zeros((n_epoch))\n",
    "    errors_train = np.zeros((n_epoch))\n",
    "    losses_test = np.zeros((n_epoch))\n",
    "    errors_test = np.zeros((n_epoch))\n",
    "    \n",
    "    # choose cross entropy loss function (equation 5.24)\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "    # construct SGD optimizer and initialize learning rate and momentum\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = 0.05, momentum=0.9)\n",
    "    # object that decreases learning rate by half every 10 epochs\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "\n",
    "    for epoch in range(n_epoch):\n",
    "      # loop over batches\n",
    "      for i, batch in enumerate(data_loader):\n",
    "        # retrieve inputs and labels for this batch\n",
    "        x_batch, y_batch = batch\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass -- calculate model output\n",
    "        pred = model(x_batch)\n",
    "        # compute the loss\n",
    "        loss = loss_function(pred, y_batch)\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        # SGD update\n",
    "        optimizer.step()\n",
    "\n",
    "      # Run whole dataset to get statistics -- normally wouldn't do this\n",
    "      pred_train = model(x_train)\n",
    "      pred_test = model(x_test)\n",
    "      _, predicted_train_class = torch.max(pred_train.data, 1)\n",
    "      _, predicted_test_class = torch.max(pred_test.data, 1)\n",
    "      errors_train[epoch] = 100 - 100 * (predicted_train_class == y_train).float().sum() / len(y_train)\n",
    "      errors_test[epoch]= 100 - 100 * (predicted_test_class == y_test).float().sum() / len(y_test)\n",
    "      losses_train[epoch] = loss_function(pred_train, y_train).item()\n",
    "      losses_test[epoch]= loss_function(pred_test, y_test).item()\n",
    "      print(f'Epoch {epoch:5d}, train loss {losses_train[epoch]:.6f}, train error {errors_train[epoch]:3.2f},  test loss {losses_test[epoch]:.6f}, test error {errors_test[epoch]:3.2f}')\n",
    "\n",
    "      # tell scheduler to consider updating learning rate\n",
    "      scheduler.step()\n",
    "        \n",
    "    return errors_train, errors_test, losses_train, losses_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "executionInfo": {
     "elapsed": 491,
     "status": "ok",
     "timestamp": 1721939019917,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "k_TzmBOVnlSL",
    "outputId": "f814cb44-a73e-440f-cdf1-0e2710316e02"
   },
   "outputs": [],
   "source": [
    "def plot_results(errors_train, errors_test, losses_train, losses_test, n_epoch=50):\n",
    "\n",
    "    # Plot the results\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(errors_train,'r-',label='train')\n",
    "    ax.plot(errors_test,'b-',label='test')\n",
    "    ax.set_ylim(0,100); ax.set_xlim(0,n_epoch)\n",
    "    ax.set_xlabel('Epoch'); ax.set_ylabel('Error')\n",
    "    ax.set_title('TrainError %3.2f, Test Error %3.2f'%(errors_train[-1],errors_test[-1]))\n",
    "    ax.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the results\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(losses_train,'r-',label='train')\n",
    "    ax.plot(losses_test,'b-',label='test')\n",
    "    ax.set_xlim(0,n_epoch)\n",
    "    ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
    "    ax.set_title('Train loss %3.2f, Test loss %3.2f'%(losses_train[-1],losses_test[-1]))\n",
    "    ax.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Train your model for 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Plot the resulting quantities:\n",
    "- Train errors\n",
    "- Test errors\n",
    "- Train losses\n",
    "- Test losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slNQbGY8uYiX"
   },
   "source": [
    "### Part 1.b)\n",
    "\n",
    "Now let's increase the capacity of our model to have five hidden layers and re-run our experiment. Pay attention to what happens to the train and test curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CSPCfIoTvU4g"
   },
   "source": [
    "**TODO:**\n",
    "Define a model with five hidden layers of size 100\n",
    "and ReLU activations between them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1721939019917,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "gy2gTmeruiz7"
   },
   "outputs": [],
   "source": [
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lx0MKjx3vbLs"
   },
   "source": [
    "**TODO**: Train the model using the same code as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLPELEolvnxS"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC3feLw5v7pd"
   },
   "source": [
    "**TODO**: Plot the results using the same code as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0QV9y79Mv-Fk"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NKPGfSmuwcbS"
   },
   "source": [
    "### Part 1.c)\n",
    "\n",
    "Let's run one last experiment, this time with a simple linear model with **NO** hidden layers. In this case we will decrease the capacity of the model. Again, pay attention to what happens to the train and test curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dDOtAsSYwtD2"
   },
   "source": [
    "**TODO:** Define a model with no hidden layers. This model should just be a linear layer that maps from the input dimension, `D_i` to the output dimension, `D_o`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1721939029187,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "OpKrzL2Dw9Gt"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f2KDpACWxJ43"
   },
   "source": [
    "**TODO**: Train the model using the same code as above\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "88LzTORexQfX"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RnmXf9ZVxUum"
   },
   "source": [
    "**TODO**: Plot the results using the same code as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4w2DbeqWyNDr"
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GhYWl39Hn2kx"
   },
   "source": [
    "### Part 1.d: Discussion\n",
    "What did you observe as you changed the model capacity in your experiments? What are some measures you might include to improve generalization of your models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your response here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n_qzCupZxv56"
   },
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWdgJdgGy163"
   },
   "source": [
    "## Q2:  Bias-variance Trade-off\n",
    "\n",
    "In this problem, we will investigate the bias-variance trade-off and reproduce the curves seen in Figure 8.8 in Prince."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1721939033578,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "biNd-0BmzB8S"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FyyvXug7zSJT"
   },
   "source": [
    "Let's specify the true function that we are trying to estimate, defined on [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1721939033578,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "dp_hhiFHzN1j"
   },
   "outputs": [],
   "source": [
    "def true_function(x):\n",
    "    y = np.exp(np.sin(x*(2*3.1413)))\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTz8KoI9zV20"
   },
   "source": [
    "Now let's generate some data point and add bit of noise to the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1721939033578,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "RibUkvvBzUkC"
   },
   "outputs": [],
   "source": [
    "def generate_data(n_data, sigma_y=0.3):\n",
    "    # Generate x values quasi uniformly\n",
    "    x = np.ones(n_data)\n",
    "    for i in range(n_data):\n",
    "        x[i] = np.random.uniform(i/n_data, (i+1)/n_data, 1)\n",
    "\n",
    "    # y value from running through function and adding noise\n",
    "    y = np.ones(n_data)\n",
    "    for i in range(n_data):\n",
    "        y[i] = true_function(x[i])\n",
    "        y[i] += np.random.normal(0, sigma_y, 1)\n",
    "    return x,y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc-ywywAzf6O"
   },
   "source": [
    "Let's draw the fitted function, together with uncertainty used to generate points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1721939033578,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "pIPVYxWEzfJ5"
   },
   "outputs": [],
   "source": [
    "def plot_function(x_func, y_func, x_data=None,y_data=None, x_model = None, y_model =None, sigma_func = None, sigma_model=None):\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "    ax.plot(x_func, y_func, 'k-')\n",
    "    if sigma_func is not None:\n",
    "      ax.fill_between(x_func, y_func-2*sigma_func, y_func+2*sigma_func, color='lightgray')\n",
    "\n",
    "    if x_data is not None:\n",
    "        ax.plot(x_data, y_data, 'o', color='#d18362')\n",
    "\n",
    "    if x_model is not None:\n",
    "        ax.plot(x_model, y_model, '-', color='#7fe7de')\n",
    "\n",
    "    if sigma_model is not None:\n",
    "      ax.fill_between(x_model, y_model-2*sigma_model, y_model+2*sigma_model, color='lightgray')\n",
    "\n",
    "    ax.set_xlim(0,1)\n",
    "    ax.set_xlabel('Input, x')\n",
    "    ax.set_ylabel('Output, y')\n",
    "    plt.show()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p14Ig5cNzxru"
   },
   "source": [
    "Sample from the true function (no noise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1721939033578,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "pMBVcLwNzjmn"
   },
   "outputs": [],
   "source": [
    "x_func = np.linspace(0, 1.0, 100)\n",
    "y_func = true_function(x_func);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UyzfDRwz5e1"
   },
   "source": [
    "Generate some training data (with noise):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1721939033578,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "LqnTSqluz-am",
    "outputId": "54555407-fc70-4273-9013-531c81d026b5"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "sigma_func = 0.3\n",
    "n_data = 15\n",
    "x_data,y_data = generate_data(n_data, sigma_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3wH2zDi0H5s"
   },
   "source": [
    "Plot the data with the functions defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 420,
     "status": "ok",
     "timestamp": 1721939033996,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "_GBGX5K60Lbz",
    "outputId": "f485ce6c-7d7a-4896-cfd1-e1e858b51f30"
   },
   "outputs": [],
   "source": [
    "plot_function(x_func, y_func, x_data, y_data, sigma_func=sigma_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1721939033996,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "t8SVgFH50MMs"
   },
   "outputs": [],
   "source": [
    "# Define model -- beta is a scalar and omega has size n_hidden,1\n",
    "def network(x, beta, omega):\n",
    "    # Retrieve number of hidden units\n",
    "    n_hidden = omega.shape[0]\n",
    "\n",
    "    y = np.zeros_like(x)\n",
    "    for c_hidden in range(n_hidden):\n",
    "        # Evaluate activations based on shifted lines (figure 8.4b-d)\n",
    "        line_vals =  x  - c_hidden/n_hidden\n",
    "        h =  line_vals * (line_vals > 0)\n",
    "        # Weight activations by omega parameters and sum\n",
    "        y = y + omega[c_hidden] * h\n",
    "    # Add bias, beta\n",
    "    y = y + beta\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1721939033996,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "9A3Hwp462OcD"
   },
   "outputs": [],
   "source": [
    "# This fits the n_hidden+1 parameters (see fig 8.4a) in closed form.\n",
    "# If you have studied linear algebra, then you will know it is a least\n",
    "# squares solution of the form (A^TA)^-1A^Tb.  If you don't recognize that,\n",
    "# then just take it on trust that this gives you the best possible solution.\n",
    "def fit_model_closed_form(x,y,n_hidden):\n",
    "  n_data = len(x)\n",
    "  A = np.ones((n_data, n_hidden+1))\n",
    "  for i in range(n_data):\n",
    "      for j in range(1,n_hidden+1):\n",
    "          A[i,j] = x[i]-(j-1)/n_hidden\n",
    "          if A[i,j] < 0:\n",
    "              A[i,j] = 0;\n",
    "\n",
    "  beta_omega = np.linalg.lstsq(A, y, rcond=None)[0]\n",
    "\n",
    "  beta = beta_omega[0]\n",
    "  omega = beta_omega[1:]\n",
    "\n",
    "  return beta, omega"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 173,
     "status": "ok",
     "timestamp": 1721939034167,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "d0pMEIav2SkA",
    "outputId": "619e9051-b654-465a-8016-ca96dabace42"
   },
   "outputs": [],
   "source": [
    "# Closed form solution\n",
    "beta, omega = fit_model_closed_form(x_data,y_data,n_hidden=3)\n",
    "\n",
    "# Get prediction for model across graph range\n",
    "x_model = np.linspace(0,1,100);\n",
    "y_model = network(x_model, beta, omega)\n",
    "\n",
    "# Draw the function and the model\n",
    "plot_function(x_func, y_func, x_data,y_data, x_model, y_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lo9GjQKLftvu"
   },
   "source": [
    "### Part 2.a) Estimating the mean and variance of the model outputs, over many training runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w6m0EzZa28Nl"
   },
   "source": [
    "**TODO:** Fill in the missing pieces of this function to run the model many times with different datasets and return the mean and variance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1721939034167,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "lTSCIXEi2YqR"
   },
   "outputs": [],
   "source": [
    "def get_model_mean_variance(n_data, n_datasets, n_hidden, sigma_func):\n",
    "\n",
    "  # Create array that stores model results in rows\n",
    "  y_model_all = np.zeros((n_datasets, n_data))\n",
    "\n",
    "  for c_dataset in range(n_datasets):\n",
    "    # TODO -- Generate n_data x,y, pairs with standard deviation sigma_func\n",
    "    # Replace this line\n",
    "    x_data, y_data = None, None\n",
    "\n",
    "    # TODO -- Fit the model\n",
    "    # Replace this line:\n",
    "    beta, omega = None, None\n",
    "\n",
    "    # TODO -- Run the fitted model on x_model\n",
    "    # Replace this line\n",
    "    y_model = None\n",
    "\n",
    "    # Store the model results\n",
    "    y_model_all[c_dataset,:] = y_model\n",
    "\n",
    "  # Get mean and standard deviation of model\n",
    "  mean_model = np.mean(y_model_all,axis=0)\n",
    "  std_model = np.std(y_model_all,axis=0)\n",
    "\n",
    "  # Return the mean and standard deviation of the fitted model\n",
    "  return mean_model, std_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzMkq7KrfVpf"
   },
   "source": [
    "Let's generate N=100 random data sets, fit the model N=100 times and look the mean and variance. Here we will have 15 data points and 3 hidden units in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1721939034167,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "SVA-Fsek2awb",
    "outputId": "9773f406-38f3-403b-98ff-da8ba199ffc5"
   },
   "outputs": [],
   "source": [
    "n_datasets = 100\n",
    "n_data = 15\n",
    "sigma_func = 0.3\n",
    "n_hidden = 3\n",
    "\n",
    "# Get mean and variance of fitted model\n",
    "np.random.seed(1)\n",
    "mean_model, std_model = get_model_mean_variance(n_data, n_datasets, n_hidden, sigma_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKXyiLhufnKy"
   },
   "source": [
    "Plot the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 449
    },
    "executionInfo": {
     "elapsed": 315,
     "status": "ok",
     "timestamp": 1721939034478,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "Z_sVRJFM59Il",
    "outputId": "8e185411-d057-419d-a227-a92acb37adab"
   },
   "outputs": [],
   "source": [
    "x_model_grid = np.linspace(0, 1, len(mean_model))\n",
    "plot_function(x_func, y_func, x_model=x_model_grid, y_model=mean_model, sigma_model=std_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eh4WZfS1eLCf"
   },
   "source": [
    "If you did this correctly, you can see that there that we observe both **bias** and **variance** in the model outputs. Here bias refers to the fact that we have some error from the model outputs and the true function (distance between cyan and black lines); variance refers to the gray region indicating there is a fair amount of variability in what the model outputs over each dataset it sees.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwK2BJaTgfEf"
   },
   "source": [
    "### Part 2.b) Changing the amount of available data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fmgb8vk9gkmq"
   },
   "source": [
    "**TODO**: Let's rerun the same experiment as the cell above, but this time let's increase the the number of training points to 100, `n_data=100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 131,
     "status": "ok",
     "timestamp": 1721939384088,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "TocjrVyMgjv-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfwYl0RFg5RB"
   },
   "source": [
    "**TODO**: Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 232,
     "status": "ok",
     "timestamp": 1721939388329,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "C22lkN1Fg6vN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IQ85NPNJhErP"
   },
   "source": [
    "**TODO**: What happened to the variance? Record your observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILpVwLAchNIE"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IZbtyZAdhU3n"
   },
   "source": [
    "### Part 2.c) Increasing the model capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PELiw9OmhgZI"
   },
   "source": [
    "**TODO**: Let's rerun the same experiment as the cell above, but this time let's increase set the number of hidden units to 12 and the number of training points to 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 149,
     "status": "ok",
     "timestamp": 1721939398242,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "qzObr8GtiHAC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HinKPGgQiQjd"
   },
   "source": [
    "**TODO**: Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 204,
     "status": "ok",
     "timestamp": 1721939400552,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "2Qr9NuZFiUab"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddt1HRY8iXSa"
   },
   "source": [
    "**TODO**: What happened to the bias? Record your observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "STzzHzUXibS9"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8-q3uKaPibZd"
   },
   "source": [
    "### Part 1d) High capacity and high data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0ADRLw4i4cO"
   },
   "source": [
    "**TODO**: Let's rerun the same experiment as the cell above, but this time let's increase set the number of hidden units to 12 and the number of training points to 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 143,
     "status": "ok",
     "timestamp": 1721939413430,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "6EWpipC2i72U"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C7AJjSURi-Qx"
   },
   "source": [
    "**TODO:** Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 132,
     "status": "ok",
     "timestamp": 1721939427100,
     "user": {
      "displayName": "Porter Jenkins",
      "userId": "02898551491497513068"
     },
     "user_tz": 360
    },
    "id": "TwFGSKRIjHgd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdwDgFvrjFjV"
   },
   "source": [
    "**TODO**: Record your observations about the bias and variance in this setting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7z1Hf1QJjQKg"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WmyrvE3Ljzt7"
   },
   "source": [
    "# Q3) L2 Regularization\n",
    "This question investigates adding L2 regularization to the loss function for the Gabor model as in figure 9.1 of Prince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our training data 30 pairs `{x_i, y_i}`. We'll try to fit the Gabor model to these data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([[-1.920e+00,-1.422e+01,1.490e+00,-1.940e+00,-2.389e+00,-5.090e+00,\n",
    "                 -8.861e+00,3.578e+00,-6.010e+00,-6.995e+00,3.634e+00,8.743e-01,\n",
    "                 -1.096e+01,4.073e-01,-9.467e+00,8.560e+00,1.062e+01,-1.729e-01,\n",
    "                  1.040e+01,-1.261e+01,1.574e-01,-1.304e+01,-2.156e+00,-1.210e+01,\n",
    "                 -1.119e+01,2.902e+00,-8.220e+00,-1.179e+01,-8.391e+00,-4.505e+00],\n",
    "                  [-1.051e+00,-2.482e-02,8.896e-01,-4.943e-01,-9.371e-01,4.306e-01,\n",
    "                  9.577e-03,-7.944e-02 ,1.624e-01,-2.682e-01,-3.129e-01,8.303e-01,\n",
    "                  -2.365e-02,5.098e-01,-2.777e-01,3.367e-01,1.927e-01,-2.222e-01,\n",
    "                  6.352e-02,6.888e-03,3.224e-02,1.091e-02,-5.706e-01,-5.258e-02,\n",
    "                  -3.666e-02,1.709e-01,-4.805e-02,2.008e-01,-1.904e-01,5.952e-01]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.a) Setting up the Gabor Model\n",
    "Gabor model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model(phi,x):\n",
    "  sin_component = np.sin(phi[0] + 0.06 * phi[1] * x)\n",
    "  gauss_component = np.exp(-(phi[0] + 0.06 * phi[1] * x) * (phi[0] + 0.06 * phi[1] * x) / 32)\n",
    "  y_pred= sin_component * gauss_component\n",
    "  return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a function to plot the data, the model and its outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_model(data,model,phi,title=None):\n",
    "  x_model = np.arange(-15,15,0.1)\n",
    "  y_model = model(phi,x_model)\n",
    "\n",
    "  fix, ax = plt.subplots()\n",
    "  ax.plot(data[0,:],data[1,:],'bo')\n",
    "  ax.plot(x_model,y_model,'m-')\n",
    "  ax.set_xlim([-15,15]);ax.set_ylim([-1,1])\n",
    "  ax.set_xlabel('x'); ax.set_ylabel('y')\n",
    "  if title is not None:\n",
    "    ax.set_title(title)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = np.zeros((2,1))\n",
    "phi[0] =  -5     # Horizontal offset\n",
    "phi[1] =  25     # Frequency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_model(data,model,phi, \"Initial parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the sum of squares loss for the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **TODO:** Implement the sum of squares loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(data_x, data_y, model, phi):\n",
    "  pred_y = model(phi, data_x)\n",
    "  # TODO: add code here to compute the loss\n",
    "  loss = None\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the whole loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pretty colormap\n",
    "my_colormap_vals_hex =('2a0902', '2b0a03', '2c0b04', '2d0c05', '2e0c06', '2f0d07', '300d08', '310e09', '320f0a', '330f0b', '34100b', '35110c', '36110d', '37120e', '38120f', '39130f', '3a1410', '3b1411', '3c1511', '3d1612', '3e1613', '3f1713', '401714', '411814', '421915', '431915', '451a16', '461b16', '471b17', '481c17', '491d18', '4a1d18', '4b1e19', '4c1f19', '4d1f1a', '4e201b', '50211b', '51211c', '52221c', '53231d', '54231d', '55241e', '56251e', '57261f', '58261f', '592720', '5b2821', '5c2821', '5d2922', '5e2a22', '5f2b23', '602b23', '612c24', '622d25', '632e25', '652e26', '662f26', '673027', '683027', '693128', '6a3229', '6b3329', '6c342a', '6d342a', '6f352b', '70362c', '71372c', '72372d', '73382e', '74392e', '753a2f', '763a2f', '773b30', '783c31', '7a3d31', '7b3e32', '7c3e33', '7d3f33', '7e4034', '7f4134', '804235', '814236', '824336', '834437', '854538', '864638', '874739', '88473a', '89483a', '8a493b', '8b4a3c', '8c4b3c', '8d4c3d', '8e4c3e', '8f4d3f', '904e3f', '924f40', '935041', '945141', '955242', '965343', '975343', '985444', '995545', '9a5646', '9b5746', '9c5847', '9d5948', '9e5a49', '9f5a49', 'a05b4a', 'a15c4b', 'a35d4b', 'a45e4c', 'a55f4d', 'a6604e', 'a7614e', 'a8624f', 'a96350', 'aa6451', 'ab6552', 'ac6552', 'ad6653', 'ae6754', 'af6855', 'b06955', 'b16a56', 'b26b57', 'b36c58', 'b46d59', 'b56e59', 'b66f5a', 'b7705b', 'b8715c', 'b9725d', 'ba735d', 'bb745e', 'bc755f', 'bd7660', 'be7761', 'bf7862', 'c07962', 'c17a63', 'c27b64', 'c27c65', 'c37d66', 'c47e67', 'c57f68', 'c68068', 'c78169', 'c8826a', 'c9836b', 'ca846c', 'cb856d', 'cc866e', 'cd876f', 'ce886f', 'ce8970', 'cf8a71', 'd08b72', 'd18c73', 'd28d74', 'd38e75', 'd48f76', 'd59077', 'd59178', 'd69279', 'd7937a', 'd8957b', 'd9967b', 'da977c', 'da987d', 'db997e', 'dc9a7f', 'dd9b80', 'de9c81', 'de9d82', 'df9e83', 'e09f84', 'e1a185', 'e2a286', 'e2a387', 'e3a488', 'e4a589', 'e5a68a', 'e5a78b', 'e6a88c', 'e7aa8d', 'e7ab8e', 'e8ac8f', 'e9ad90', 'eaae91', 'eaaf92', 'ebb093', 'ecb295', 'ecb396', 'edb497', 'eeb598', 'eeb699', 'efb79a', 'efb99b', 'f0ba9c', 'f1bb9d', 'f1bc9e', 'f2bd9f', 'f2bfa1', 'f3c0a2', 'f3c1a3', 'f4c2a4', 'f5c3a5', 'f5c5a6', 'f6c6a7', 'f6c7a8', 'f7c8aa', 'f7c9ab', 'f8cbac', 'f8ccad', 'f8cdae', 'f9ceb0', 'f9d0b1', 'fad1b2', 'fad2b3', 'fbd3b4', 'fbd5b6', 'fbd6b7', 'fcd7b8', 'fcd8b9', 'fcdaba', 'fddbbc', 'fddcbd', 'fddebe', 'fddfbf', 'fee0c1', 'fee1c2', 'fee3c3', 'fee4c5', 'ffe5c6', 'ffe7c7', 'ffe8c9', 'ffe9ca', 'ffebcb', 'ffeccd', 'ffedce', 'ffefcf', 'fff0d1', 'fff2d2', 'fff3d3', 'fff4d5', 'fff6d6', 'fff7d8', 'fff8d9', 'fffada', 'fffbdc', 'fffcdd', 'fffedf', 'ffffe0')\n",
    "my_colormap_vals_dec = np.array([int(element,base=16) for element in my_colormap_vals_hex])\n",
    "r = np.floor(my_colormap_vals_dec/(256*256))\n",
    "g = np.floor((my_colormap_vals_dec - r *256 *256)/256)\n",
    "b = np.floor(my_colormap_vals_dec - r * 256 *256 - g * 256)\n",
    "my_colormap = ListedColormap(np.vstack((r,g,b)).transpose()/255.0)\n",
    "\n",
    "def draw_loss_function(compute_loss, data,  model, my_colormap, phi_iters = None):\n",
    "\n",
    "  # Make grid of offset/frequency values to plot\n",
    "  offsets_mesh, freqs_mesh = np.meshgrid(np.arange(-10,10.0,0.1), np.arange(2.5,22.5,0.1))\n",
    "  loss_mesh = np.zeros_like(freqs_mesh)\n",
    "  # Compute loss for every set of parameters\n",
    "  for idslope, slope in np.ndenumerate(freqs_mesh):\n",
    "     loss_mesh[idslope] = compute_loss(data[0,:], data[1,:], model, np.array([[offsets_mesh[idslope]], [slope]]))\n",
    "\n",
    "  fig,ax = plt.subplots()\n",
    "  fig.set_size_inches(8,8)\n",
    "  ax.contourf(offsets_mesh,freqs_mesh,loss_mesh,256,cmap=my_colormap)\n",
    "  ax.contour(offsets_mesh,freqs_mesh,loss_mesh,20,colors=['#80808080'])\n",
    "  if phi_iters is not None:\n",
    "    ax.plot(phi_iters[0,:], phi_iters[1,:],'go-')\n",
    "  ax.set_ylim([2.5,22.5])\n",
    "  ax.set_xlabel('Offset $\\phi_{0}$'); ax.set_ylabel('Frequency, $\\phi_{1}$')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss_function(compute_loss, data, model, my_colormap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the gradient vector for a given set of parameters:\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial L}{\\partial \\boldsymbol\\phi} = \\begin{bmatrix}\\frac{\\partial L}{\\partial \\phi_0} \\\\\\frac{\\partial L}{\\partial \\phi_1} \\end{bmatrix}.\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gabor model has two parameters, $\\phi_0$ and $\\phi_1$. We need to comptue the deriviate of the loss (sum of squares) with respect both quantities. Below we define two functions that each implement of the two partial deriviates we need. This is a hassle to get right!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabor_deriv_phi0(data_x,data_y,phi0, phi1):\n",
    "    x = 0.06 * phi1 * data_x + phi0\n",
    "    y = data_y\n",
    "    cos_component = np.cos(x)\n",
    "    sin_component = np.sin(x)\n",
    "    gauss_component = np.exp(-0.5 * x *x / 16)\n",
    "    deriv = cos_component * gauss_component - sin_component * gauss_component * x / 16\n",
    "    deriv = 2* deriv * (sin_component * gauss_component - y)\n",
    "    return np.sum(deriv)\n",
    "\n",
    "def gabor_deriv_phi1(data_x, data_y,phi0, phi1):\n",
    "    x = 0.06 * phi1 * data_x + phi0\n",
    "    y = data_y\n",
    "    cos_component = np.cos(x)\n",
    "    sin_component = np.sin(x)\n",
    "    gauss_component = np.exp(-0.5 * x *x / 16)\n",
    "    deriv = 0.06 * data_x * cos_component * gauss_component - 0.06 * data_x*sin_component * gauss_component * x / 16\n",
    "    deriv = 2*deriv * (sin_component * gauss_component - y)\n",
    "    return np.sum(deriv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's implement a function that puts each partial deriviate into a vector, called a gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(data_x, data_y, phi):\n",
    "    dl_dphi0 = gabor_deriv_phi0(data_x, data_y, phi[0],phi[1])\n",
    "    dl_dphi1 = gabor_deriv_phi1(data_x, data_y, phi[0],phi[1])\n",
    "    # Return the gradient\n",
    "    return np.array([[dl_dphi0],[dl_dphi1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to find the minimum.  For simplicity, we'll just use regular (non-stochastic) gradient descent with a fixed learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Implement the gradient descent updates in the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step(phi, data):\n",
    "  # TODO: Implement the gradient descent step\n",
    "  # Step 1:  Compute the gradient\n",
    "  # Step 2:  Update the parameters -- note we want to search in the negative (downhill direction)\n",
    "  alpha = 0.1 # learning rate\n",
    "  phi = None\n",
    "  return phi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the parameters\n",
    "n_steps = 41\n",
    "phi_all = np.zeros((2,n_steps+1))\n",
    "phi_all[0,0] = 2.6\n",
    "phi_all[1,0] = 8.5\n",
    "\n",
    "# Measure loss and draw initial model\n",
    "loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,0:1])\n",
    "draw_model(data,model,phi_all[:,0:1], \"Initial parameters, Loss = %f\"%(loss))\n",
    "\n",
    "for c_step in range (n_steps):\n",
    "  # Do gradient descent step\n",
    "  phi_all[:,c_step+1:c_step+2] = gradient_descent_step(phi_all[:,c_step:c_step+1],data)\n",
    "  # Measure loss and draw model every 8th step\n",
    "  if c_step % 8 == 0:\n",
    "    loss =  compute_loss(data[0,:], data[1,:], model, phi_all[:,c_step+1:c_step+2])\n",
    "    draw_model(data,model,phi_all[:,c_step+1], \"Iteration %d, loss = %f\"%(c_step+1,loss))\n",
    "\n",
    "draw_loss_function(compute_loss, data, model, my_colormap, phi_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3.b) Applying L2 Regularization\n",
    "\n",
    "Unfortunately, when we start from this position, the solution descends to a local minimum and the final model doesn't fit well.<br><br>\n",
    "\n",
    "But what if we had some weak knowledge that the solution was in the vicinity of $\\phi_0=0.0$, $\\phi_{1} = 12.5$ (the center of the plot)?\n",
    "\n",
    "Let's add a term to the loss function that penalizes solutions that deviate from this point.  \n",
    "\n",
    "\\begin{equation}\n",
    "L'[\\boldsymbol\\phi] = L[\\boldsymbol\\phi]+ \\lambda\\cdot \\Bigl(\\phi_{0}^2+(\\phi_1-12.5)^2\\Bigr)\n",
    "\\end{equation}\n",
    "\n",
    "where $\\lambda$ controls the relative importance of the original loss and the regularization term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: define a function that implements the L2 regularization term (term in large parentheses in the above equation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the regularization term\n",
    "def compute_reg_term(phi0,phi1):\n",
    "  # TODO Replace this line\n",
    "  reg_term = None\n",
    "  return reg_term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Now define the whole loss function, which is the sum of squares plus $\\lambda$ times the L2 regularizatoin term, or $L'[\\phi]$ defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note I called the weighting lambda_ to avoid confusing it with python lambda functions\n",
    "def compute_loss2(data_x, data_y, model, phi, lambda_):\n",
    "  pred_y = model(phi, data_x)\n",
    "  # TODO: define the loss function\n",
    "  loss = None\n",
    "  return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to draw the regularization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_reg_function():\n",
    "\n",
    "  # Make grid of offset/frequency values to plot\n",
    "  offsets_mesh, freqs_mesh = np.meshgrid(np.arange(-10,10.0,0.1), np.arange(2.5,22.5,0.1))\n",
    "  loss_mesh = np.zeros_like(freqs_mesh)\n",
    "  # Compute loss for every set of parameters\n",
    "  for idslope, slope in np.ndenumerate(freqs_mesh):\n",
    "     loss_mesh[idslope] = compute_reg_term(offsets_mesh[idslope], slope)\n",
    "\n",
    "  fig,ax = plt.subplots()\n",
    "  fig.set_size_inches(8,8)\n",
    "  ax.contourf(offsets_mesh,freqs_mesh,loss_mesh,256,cmap=my_colormap)\n",
    "  ax.contour(offsets_mesh,freqs_mesh,loss_mesh,20,colors=['#80808080'])\n",
    "  ax.set_ylim([2.5,22.5])\n",
    "  ax.set_xlabel('Offset $\\phi_{0}$'); ax.set_ylabel('Frequency, $\\phi_{1}$')\n",
    "  plt.show()\n",
    "\n",
    "# Draw the regularization function.  It should look similar to figure 9.1b\n",
    "draw_reg_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to draw loss function with regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_loss_function_reg(data,  model, lambda_, my_colormap, phi_iters = None):\n",
    "\n",
    "  # Make grid of offset/frequency values to plot\n",
    "  offsets_mesh, freqs_mesh = np.meshgrid(np.arange(-10,10.0,0.1), np.arange(2.5,22.5,0.1))\n",
    "  loss_mesh = np.zeros_like(freqs_mesh)\n",
    "  # Compute loss for every set of parameters\n",
    "  for idslope, slope in np.ndenumerate(freqs_mesh):\n",
    "     loss_mesh[idslope] = compute_loss2(data[0,:], data[1,:], model, np.array([[offsets_mesh[idslope]], [slope]]), lambda_)\n",
    "\n",
    "  fig,ax = plt.subplots()\n",
    "  fig.set_size_inches(8,8)\n",
    "  ax.contourf(offsets_mesh,freqs_mesh,loss_mesh,256,cmap=my_colormap)\n",
    "  ax.contour(offsets_mesh,freqs_mesh,loss_mesh,20,colors=['#80808080'])\n",
    "  if phi_iters is not None:\n",
    "    ax.plot(phi_iters[0,:], phi_iters[1,:],'go-')\n",
    "  ax.set_ylim([2.5,22.5])\n",
    "  ax.set_xlabel('Offset $\\phi_{0}$'); ax.set_ylabel('Frequency, $\\phi_{1}$')\n",
    "  plt.show()\n",
    "\n",
    "# This should look something like figure 9.1c\n",
    "draw_loss_function_reg(data, model, 0.2, my_colormap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll compute the derivatives $\\frac{\\partial L'}{\\partial\\phi_0}$ and $\\frac{\\partial L'}{\\partial\\phi_1}$ of the regularized loss function:\n",
    "\n",
    "\\begin{equation}\n",
    "L'[\\boldsymbol\\phi] = L[\\boldsymbol\\phi]+ \\lambda\\cdot \\Bigl(\\phi_{0}^2+(\\phi_1-12.5)^2\\Bigr)\n",
    "\\end{equation}\n",
    "\n",
    "so that we can perform gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Implement the partial of deriviates each the regularized loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient2(data_x, data_y, phi, lambda_):\n",
    "    # TODO: compute partial derivates of each phi here:\n",
    "    dl_dphi0 = None\n",
    "    dl_dphi1 = None\n",
    "    # Return the gradient\n",
    "    return np.array([dl_dphi0, dl_dphi1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** Implement the gradient descent step with the regularized loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_step2(phi, lambda_, data):\n",
    "  # TODO: Implement gradient descent\n",
    "  # Step 1:  Compute the gradient\n",
    "  gradient = None\n",
    "  # Step 2:  Update the parameters -- note we want to search in the negative (downhill direction)\n",
    "  alpha = 0.1\n",
    "  phi = None\n",
    "  return phi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's run gradient descent and draw the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 41\n",
    "phi_all = np.zeros((2,n_steps+1))\n",
    "phi_all[0,0] = 2.6\n",
    "phi_all[1,0] = 8.5\n",
    "lambda_ = 0.2\n",
    "\n",
    "# Measure loss and draw initial model\n",
    "loss =  compute_loss2(data[0,:], data[1,:], model, phi_all[:,0:1], lambda_)\n",
    "draw_model(data,model,phi_all[:,0:1], \"Initial parameters, Loss = %f\"%(loss))\n",
    "\n",
    "for c_step in range (n_steps):\n",
    "  # Do gradient descent step\n",
    "  phi_all[:,c_step+1:c_step+2] = gradient_descent_step2(phi_all[:,c_step:c_step+1],lambda_, data)\n",
    "  # Measure loss and draw model every 8th step\n",
    "  if c_step % 8 == 0:\n",
    "    loss =  compute_loss2(data[0,:], data[1,:], model, phi_all[:,c_step+1:c_step+2], lambda_)\n",
    "    draw_model(data,model,phi_all[:,c_step+1], \"Iteration %d, loss = %f\"%(c_step+1,loss))\n",
    "\n",
    "draw_loss_function_reg(data, model, lambda_, my_colormap, phi_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that the gradient descent algorithm now finds the correct minimum. By applying a tiny bit of domain knowledge (the parameter phi0 tends to be near zero and the parameter phi1 tends to be near 12.5), we get a better solution. However, the cost is that this solution is slightly biased towards this prior knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.c) Low regularization\n",
    "Let's experiment with different values of the regularization weight `lambda_`. In this experiment. Set `lambda_=0.01`. What happens when the regularization value is small? What happens to the final loss value when we add the regularization term? Does it go up?  Go down?  Stay the same? Record your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3.d) High regularization\n",
    "\n",
    "Now set `lambda_=1.0`. What happens when the regularization value is large. What happens to the final loss value when we add the regularization term? Does it go up?  Go down?  Stay the same? Record your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q4) Overfitting to MNIST\n",
    "In this question, we will train a ConvNet on the MNIST dataset. The model will contain many more parameters than data points. We will investigate how to address the overfitting problem with L2 regularization (weight decay)\n",
    "\n",
    "The MNISt dataset contains cropped images of handwritten digits from 0 to 9. Our task will be to classify the input image to the correct digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will want to make sure that you're connected to a GPU instance for this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your device should be `device(type='cuda')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's specify our hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And setup our transforms. The following code will take an input image, cast it to a tensor and normalize it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's download the MNIST dataset and create our dataloader object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write some code to visualize one of the images from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_dataset.data[0], cmap='gray')\n",
    "plt.title('%i' % train_dataset.targets[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot a bunch of digits together so you get an idea of what the training data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure = plt.figure(figsize=(10, 8))\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
    "    img, label = train_dataset[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(label)\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.a) Training a ConvNet on MNIST\n",
    "\n",
    "**TODO**: Here we will specify our CNN model. You will need to create a model class, called `CNN`, which inherits from the `nn.Module` base class.\n",
    "\n",
    "The CNN should have the following components:\n",
    "\n",
    "\n",
    "*   A `Conv2D` layer with 1 input channel, 32 output channels, a kernel size of 2 a stride of 1, and a padding of 3 pixels\n",
    "*   After the first `Conv2D` layer you should apply a `ReLU` acvtivation\n",
    "*   Then apply `MaxPool2d` with a kernel size of 2\n",
    "*   Next, you'll apply a second `Conv2D` layer with input channel of 32, output channel of 64, kernel size of 3, stride of 1, and padding of 2\n",
    "*   Apply another `ReLU` activation\n",
    "*   Introduce another `MaxPool2d` again with a kernel size of 2\n",
    "*   Next, you will flatten the output of the maxpool layer into a tensor with dimensions `batch_size x (64 * 9 * 9)`, where 64 is the number of output channgel from the previous `Conv2D` layer and 9 is the remaining spatial dimensions after applying maxpooling.\n",
    "*   Next, introduce a linear layer that maps from `(64 * 9 * 9)` to 1000\n",
    "*   Finally, apply a final, linear layer that maps from `1000` hidden units to `10` the number of classes in the dataset\n",
    "*   Note: we do not need to apply the softmax operation because the `nn.CrossEntropyLoss()` module requires unscaled logits for numerical stability. We will apply the argmax function in the test loop to evaluate accuracy.\n",
    "*   If you have implemented your model correctly, you should have 5,213,666 total parameters\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # TODO: Your code here\n",
    "\n",
    "    def forward(self, x):\n",
    "        # TODO: Your code here\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And count the number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "num_params = count_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've implemented a test to make sure you've implemented your model correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert num_params == 5213666, f\"Expected 5213666, got {num_params} parameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Total number of parameters: {num_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of training examples: {len(train_loader.dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Compute the ratio of parameters to data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fill this line here\n",
    "ratio =  None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have {:.2f}x more parameters than data points!\".format(ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Write 1-2 sentences explaining why the ratio of parameters to data points is difficult for learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate our optimizer and loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is our training and test loop. Let's train our model to see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "trn_losses = []\n",
    "test_losses = []\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting epoch: {epoch + 1}\")\n",
    "    epoch_loss = 0.0\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, (images, labels) in pbar:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "        pbar.set_postfix({'loss': f'{loss.detach().item():.4f}', 'iteration': i+1})\n",
    "    epoch_loss /= len(train_loader)\n",
    "    trn_losses.append(epoch_loss)\n",
    "\n",
    "    print(\"Starting test set loop\")\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        total_test_loss = 0\n",
    "        for images, labels in tqdm(test_loader, total=len(test_loader)):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            test_loss = criterion(outputs, labels)\n",
    "            total_test_loss += test_loss.detach().item()\n",
    "        total_test_loss /= len(test_loader)\n",
    "        print(f\"Test Loss: {total_test_loss:.4f}, Test: Accuracy: {(correct/total):.4f}\")\n",
    "        test_losses.append(total_test_loss)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are all done training. Let's visualize our train and test loss curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(np.arange(1, num_epochs+1), test_losses, linestyle='--', marker='o', label='TEST')\n",
    "plt.plot(np.arange(1, num_epochs+1), trn_losses, linestyle='--', marker='o', label='TRAIN')\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Test Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO:** What do you notice about the chart above? Is there a difference in the behavior of the train and test curves? If so, what is the difference and why do you think this is occuring?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4.b) Weight Decay\n",
    "\n",
    "Now we apply L2 regularization (often called weight decay) to our training algorithm. Fortunately, this is quite easy with the `Adam` optimizer class in PyTorch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Create a variable called `weight_decay` and set it to 0.001. Pass this into the `Adam` constructor and retrain your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 128\n",
    "learning_rate = 0.001\n",
    "num_epochs = 25\n",
    "# TODO: add your weight decay variable here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's re-instantiate our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Instantiate your optimizer object with weight decay. See the docs if you have any question: https://pytorch.org/docs/stable/generated/torch.optim.Adam.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: implement an adam optimizer with weight decay\n",
    "optimizer = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: train your model with the same setup as before. However, this time create a new list of epoch-wise loss values for the train and the test loop. In other words, rename `trn_losses` --> `wd_train_losses` and test_losses --> `wd_test_losses`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot the epoch-wise losses from our baseline experiment (with no weight decay) and the epoch-wise losses from our training run with weight decay:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(1, num_epochs+1), test_losses, linestyle='--', marker='o', label='TEST')\n",
    "plt.plot(np.arange(1, num_epochs+1), trn_losses, linestyle='--', marker='o', label='TRAIN')\n",
    "\n",
    "plt.plot(np.arange(1, num_epochs+1), wd_test_losses, linestyle='--', marker='o', label='TEST (weight decay)')\n",
    "plt.plot(np.arange(1, num_epochs+1), wd_trn_losses, linestyle='--', marker='o', label='TRAIN (weight decay)')\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training vs Test Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TODO**: Discuss your observations about 1) the behavior of the train loss in the baseline run vs the train loss in the weight decay run and 2) the behavior of the test loss in the baseline run vs the test loss of the weight decay run. Discuss whether or not you think applying weight decay was successful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMTKyxP874IvwqjA6jyCJ8e",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
